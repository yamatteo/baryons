{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c09de82-546b-4467-b756-415daf76b39f",
   "metadata": {},
   "source": [
    "## Some tests about the monolithic preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1851d3d-8c6a-45fb-afba-de1e8f3d09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark matter particles\n",
      "[[ 1.04231418e+01 -6.40194738e+00 -1.61657979e+00  4.72394449e-02]\n",
      " [ 1.13459913e+01 -5.95776642e+00 -1.21491853e+00  4.72394449e-02]\n",
      " [ 9.68148905e+00 -6.86386136e+00 -2.07751967e+00  4.72394449e-02]\n",
      " ...\n",
      " [ 3.41927230e+01  8.02075748e+01 -2.03665752e+01  4.72394449e-02]\n",
      " [-1.10674581e+02  6.11351384e+01 -1.89990026e+00  4.72394449e-02]\n",
      " [-1.61922891e+02  1.69029178e+02 -4.71163264e+00  4.72394449e-02]]\n",
      "Gas particles\n",
      "[[ 1.26990368e+01 -4.45786014e+00 -8.95605058e+00  1.22793103e-02]\n",
      " [ 9.98359735e+00 -2.65588877e+00 -1.13402028e+01  6.87742047e-03]\n",
      " [ 1.86737134e+01 -2.95313115e+00 -1.08345178e+01  1.32554537e-02]\n",
      " ...\n",
      " [ 5.34092582e+01 -8.21173002e+01 -6.43312808e+01  5.61451446e-03]\n",
      " [ 6.79069628e+01 -8.59964206e+01 -6.93208099e+01  8.22646916e-03]\n",
      " [-7.93876567e+01  1.17252220e+02  1.54833089e+01  7.14536477e-03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from preprocessing import find_ids, find_halo\n",
    "\n",
    "# First utility of the module is the find_ids function. It returns an array of integers.\n",
    "ids = find_ids(\n",
    "    base_path='dataset/TNG100-3/output',\n",
    "    hsmall=0.6774,\n",
    "    snap_num=99,\n",
    "    mass_min=1000000000000.0,\n",
    "    mass_max=5000000000000.0,\n",
    "    n_gas_min=500,\n",
    ")\n",
    "assert isinstance(ids, np.ndarray)\n",
    "assert ids.dtype == np.int64\n",
    "\n",
    "# Secondly, with a loop over all indexes, find_halo build two array of NUM_PARTICLES rows and 4 columns (X, Y, Z, MASS).\n",
    "dm_halo, gas_halo = find_halo(\n",
    "    base_path='dataset/TNG100-3/output',\n",
    "    halo_id=ids[0],\n",
    "    lbox=75000.0,\n",
    "    mdm=472394449.36521995,\n",
    "    snap_num=99\n",
    ")\n",
    "\n",
    "# X, Y and Z can be positive ore negative, because they are relative to the center of mass of the halo. Masses in `dm_halo` are always equal to `mdm / 1e10` and masses of `gas_halo` varies.\n",
    "print(\"Dark matter particles\")\n",
    "print(dm_halo)\n",
    "\n",
    "print(\"Gas particles\")\n",
    "print(gas_halo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a9100f-8137-43aa-817d-90601b8d905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of dark matter [-1264.38641317  2225.41274029  1976.13679177]\n",
      "Weighted center of dark matter [-59.72891234 105.12726261  93.35160516]\n",
      "Weighted center of gas [-27.61739674 -34.16789986 -48.68320766]\n",
      "Weighted center combined [-87.34630908  70.95936274  44.6683975 ]\n"
     ]
    }
   ],
   "source": [
    "# To test if position are really relative to the center of mass, we can sum all positions od dm_halo, because they have all the same weight.\n",
    "print(\"Center of dark matter\", np.sum(dm_halo[:, :3], axis=0))\n",
    "\n",
    "# Maybe the center of mass is calculated considering dark matter and gas together. Let's weight the positions and sum them.\n",
    "dm_wpos = (dm_halo[:,:3] * dm_halo[:,3].reshape((-1,1)))\n",
    "gas_wpos = (gas_halo[:,:3] * gas_halo[:,3].reshape((-1,1)))\n",
    "\n",
    "print(\"Weighted center of dark matter\", np.sum(dm_wpos, axis=0))\n",
    "print(\"Weighted center of gas\", np.sum(gas_wpos, axis=0))\n",
    "print(\"Weighted center combined\", np.sum(np.vstack([dm_wpos, gas_wpos]), axis=0))\n",
    "\n",
    "# Maybe weights are not the same for dark matter and gas, but no factor can sum the two negative x to 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f064d65c-245f-4bff-a9f5-9d8c46922545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10.42314178   -6.40194738   -1.61657979]\n",
      " [  11.34599131   -5.95776642   -1.21491853]\n",
      " [   9.68148905   -6.86386136   -2.07751967]\n",
      " ...\n",
      " [  34.19272299   80.20757477  -20.36657519]\n",
      " [-110.6745806    61.13513837   -1.89990026]\n",
      " [-161.92289109  169.02917842   -4.71163264]]\n"
     ]
    }
   ],
   "source": [
    "# ... so they are not the positions relative to the center of mass. Nevertheless they are positions.\n",
    "# Doing what voxelize would do we first split the input\n",
    "dm_pos, dm_mass = dm_halo[:, 0:3], dm_halo[:, 3]\n",
    "gas_pos, gas_mass = gas_halo[:, 0:3], gas_halo[:, 3]\n",
    "print(dm_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3309297-159c-445b-b05b-4789cfeebdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237.74349354 220.91840438 225.70377197]\n",
      " [238.66634307 221.36258534 226.10543323]\n",
      " [237.00184081 220.4564904  225.24283209]\n",
      " ...\n",
      " [261.51307475 307.52792653 206.95377657]\n",
      " [116.64577116 288.45549013 225.42045149]\n",
      " [ 65.39746066 396.34953017 222.60871912]]\n"
     ]
    }
   ],
   "source": [
    "# Shift positions to have no negatives\n",
    "minimum = min(np.min(dm_pos), np.min(gas_pos))\n",
    "dm_pos = dm_pos - minimum\n",
    "gas_pos = gas_pos - minimum\n",
    "print(dm_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd6c168-69a8-4a08-86da-fd308abfecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[129 120 122]\n",
      " [130 120 123]\n",
      " [129 120 122]\n",
      " ...\n",
      " [142 167 112]\n",
      " [ 63 157 122]\n",
      " [ 35 215 121]]\n"
     ]
    }
   ],
   "source": [
    "# Resize to nvoxel and floor\n",
    "nvoxel = 256\n",
    "maximum = max(np.max(dm_pos), np.max(gas_pos))\n",
    "dm_pos = np.floor(dm_pos * (nvoxel - 1) / maximum).astype(np.int64)\n",
    "gas_pos = np.floor(gas_pos * (nvoxel - 1) / maximum).astype(np.int64)\n",
    "print(dm_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10ff079-a540-4b70-b80f-b7ba617891f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[129, 130, 129,  ..., 142,  63,  35],\n",
      "                       [120, 120, 120,  ..., 167, 157, 215],\n",
      "                       [122, 123, 122,  ..., 112, 122, 121]]),\n",
      "       values=tensor([0.0472, 0.0472, 0.0472,  ..., 0.0472, 0.0472, 0.0472]),\n",
      "       size=(256, 256, 256), nnz=7886, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "# Then voxelize take the position as indices and the masses as values of a sparse uncoalesced tensor (see https://pytorch.org/docs/stable/sparse.html#sparse-uncoalesced-coo-docs)\n",
    "# Positions are transposed because that's the way torch wants them: columns of indices are the rows of dm_pos.\n",
    "dm_uncoalesced = torch.sparse_coo_tensor(\n",
    "    indices=dm_pos.T, values=dm_mass, size=(nvoxel, nvoxel, nvoxel)\n",
    ")\n",
    "gas_uncoalesced = torch.sparse_coo_tensor(\n",
    "    indices=gas_pos.T, values=gas_mass, size=(nvoxel, nvoxel, nvoxel)\n",
    ")\n",
    "print(dm_uncoalesced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1d70a8-2b5d-457f-9521-ef00b9d5553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncoalesced:\n",
      " tensor(indices=tensor([[0, 0, 2],\n",
      "                       [1, 1, 2],\n",
      "                       [1, 1, 0]]),\n",
      "       values=tensor([1., 5., 8.]),\n",
      "       size=(4, 4, 4), nnz=3, layout=torch.sparse_coo)\n",
      "coalesced:\n",
      " tensor(indices=tensor([[0, 2],\n",
      "                       [1, 2],\n",
      "                       [1, 0]]),\n",
      "       values=tensor([6., 8.]),\n",
      "       size=(4, 4, 4), nnz=2, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "# Finally, these tensors are coalesced: values with the same index are summed. Check this out\n",
    "uncoalesced = torch.sparse_coo_tensor(\n",
    "    indices=[[0, 0, 2],\n",
    "             [1, 1, 2],\n",
    "             [1, 1, 0]],\n",
    "    values=[1.0, 5.0, 8.0],\n",
    "    size=(4, 4, 4)\n",
    ")\n",
    "coalesced = uncoalesced.coalesce()\n",
    "print(\"uncoalesced:\\n\", uncoalesced)\n",
    "print(\"coalesced:\\n\", coalesced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed68cb1-d341-482b-b762-c7071f6decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tensors are then saved as .npy and than later converted into dense tensors by the dataset.\n",
    "# In this way we avoid saving the intermediate filse about the whole unprocessed halos.\n",
    "# Storing tensors in sparse format save orders of magnitude in disk space: a tipical dark matter halo has around 1e5 particles (in TNG100-3), but a tensor of shape [256, 256, 256] has 1.67e7 elements\n",
    "# Using .npy format (which is binary) instead of .csv (which is textual) save more or less 50% of disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e670466-616d-473b-aa85-b475975a6ea8",
   "metadata": {},
   "source": [
    "## Log distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0e9e0d-b4fb-4014-9499-505793074f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "original_values = dm_uncoalesced.coalesce().values()\n",
    "grain = torch.min(original_values)\n",
    "log_values = torch.log(original_values / (grain / 2))\n",
    "log_total = torch.sum(log_values)\n",
    "log_distribution = torch.div(log_values, log_total)\n",
    "\n",
    "reconstruct = torch.exp(log_distribution * log_total) * (grain / 2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ed4bc-8ea0-492e-b0cc-67cd868c2ab3",
   "metadata": {},
   "source": [
    "## Mass predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd310e94-deee-440d-9918-a83a2618177c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'expecttest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2381/1810403085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlockworkDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogarithmicDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msim_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TNG100-3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baryons/multiscale/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch.testing._internal.distributed.rpc.examples.parameter_server_test import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/baryons-dgAQvMH4/lib/python3.9/site-packages/torch/testing/_internal/distributed/rpc/examples/parameter_server_test.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from torch.testing._internal.dist_utils import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdist_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mworker_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/baryons-dgAQvMH4/lib/python3.9/site-packages/torch/testing/_internal/dist_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_rref_context_get_debug_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFILE_SCHEMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/baryons-dgAQvMH4/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexpecttest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0m_compare_tensors_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compare_scalars_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compare_return_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'expecttest'"
     ]
    }
   ],
   "source": [
    "from options import opts\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from preprocessing import preprocess\n",
    "from multiscale.dataset import BlockworkDataset, LogarithmicDataset\n",
    "opts = Namespace(**opts)\n",
    "sim_name = \"TNG100-3\"\n",
    "snap_num = 99\n",
    "mass_min = 1e12\n",
    "mass_max = 100e12\n",
    "n_gas_min = 100\n",
    "nvoxel = 16\n",
    "simulation_base_path = \"dataset\"\n",
    "voxels_base_path = \"data\"\n",
    "voxelization_name = f\"{sim_name}_SNAP{snap_num:03d}_MASS{mass_min:.2e}_{mass_max:.2e}_NGASMIN{n_gas_min}\"\n",
    "\n",
    "_, stats = preprocess(\n",
    "    source_path=Path(simulation_base_path) / sim_name / \"output\",\n",
    "    target_path=Path(voxels_base_path) / voxelization_name,\n",
    "    sim_name=sim_name,\n",
    "    snap_num=snap_num,\n",
    "    mass_min=mass_min,\n",
    "    mass_max=mass_max,\n",
    "    n_gas_min=n_gas_min,\n",
    "    nvoxel=nvoxel,\n",
    ")\n",
    "\n",
    "print(stats)\n",
    "dataset = LogarithmicDataset(Path(voxels_base_path) / voxelization_name / f\"nvoxel_{nvoxel}\" / \"valid\")\n",
    "# grain = min([min(dataset[i][\"rg\"][0][dataset[i][\"rg\"][0]>0]) for i in range(len(dataset))])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = [ dataset[i] for i in range(len(dataset)) ]\n",
    "means = np.array([torch.mean(line[\"dm\"]) for line in data])\n",
    "vars_ = np.array([torch.var(line[\"dm\"]) for line in data])\n",
    "xs = np.stack((means, vars_), axis=-1)\n",
    "ys = np.array([torch.mean(line[\"rg\"]) for line in data])\n",
    "\n",
    "# xs = np.log(xs)\n",
    "# ys = np.log(ys)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "print(\"cov\", np.cov(means, ys))\n",
    "\n",
    "reg = LinearRegression().fit(xs, ys)\n",
    "print(reg.score(xs, ys))\n",
    "\n",
    "# reg.predict(xs), ys\n",
    "\n",
    "p = sns.scatterplot(x=vars_, y=ys)\n",
    "# p.set_ylim(2, 8)\n",
    "# fig, axs = plt.subplots(1, 3)\n",
    "# original = dataset[3][\"dm\"][0]\n",
    "# example = original.clone()\n",
    "# seed = torch.min(example[example > 0])/2\n",
    "# example = example + seed\n",
    "# example = example / seed\n",
    "# example = torch.log(example)\n",
    "# total_dm = torch.sum(example)\n",
    "# example = example / total_dm\n",
    "# sns.heatmap(example[8], ax=axs[0], cbar=True, square=True, xticklabels=False, yticklabels=False)\n",
    "# example *= total_dm\n",
    "# example = torch.exp(example)\n",
    "# example *= seed\n",
    "# example -= seed\n",
    "# sns.heatmap(example[8], ax=axs[1], cbar=True, square=True, xticklabels=False, yticklabels=False)\n",
    "# sns.heatmap(original[8], ax=axs[2], cbar=True, square=True, xticklabels=False, yticklabels=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7d069-b34e-478b-b4a1-942b16a445ca",
   "metadata": {},
   "source": [
    "## Questions of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31e818-df91-46c6-baa5-a0fe12e1a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options import opts\n",
    "from argparse import Namespace\n",
    "opts = Namespace(**opts)\n",
    "voxelization_name = f\"{opts.sim_name}_SNAP{opts.snap_num:03d}_MASS{opts.mass_min:.2e}_{opts.mass_max:.2e}_NGASMIN{opts.n_gas_min}\"\n",
    "x = torch.load(f\"data/{voxelization_name}/nvoxel_16/train/halo_3403_coalesced.npy\")[\"rg\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "len(x.coalesce().values())\n",
    "sns.set(style = \"darkgrid\")\n",
    "pos = x.coalesce().indices()\n",
    "mass = x.coalesce().values()\n",
    "print(type(mass))\n",
    "mass = mass / torch.max(mass)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pos[0], pos[1], pos[2], marker=\".\", alpha=mass)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fd540-f5e3-4001-85e9-08268c989b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"data/TNG100-3_SNAP099_MASS1.00e+12_5.00e+12_NGASMIN2000/nvoxel_16/test/halo_12152_coalesced.npy\")[\"dm\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "print(len(x.coalesce().values()))\n",
    "\n",
    "# sns.set(style = \"darkgrid\")\n",
    "# pos = x.coalesce().indices()\n",
    "# mass = x.coalesce().values()\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# ax.scatter(pos[0], pos[1], pos[2], marker=\".\", alpha=mass*0.5)\n",
    "# plt.show()\n",
    "\n",
    "print(pos.shape, mass.shape)\n",
    "max(mass), min(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e872b4-ec21-406b-b8fb-fc9fa03a68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "s = torch.load(Path(\"runs\") / \"sample_700.npy\")\n",
    "x = (s[\"rg\"][14,0,:,:,:]).detach().cpu().numpy()\n",
    "\n",
    "print(np.mean(x, axis=(0, 1)))\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "l = len(x)\n",
    "sns.set(style = \"darkgrid\")\n",
    "pos = list(itertools.product(range(0, l), range(0, l), range(0, l)))\n",
    "mass = np.array([ x[p] for p in pos ])\n",
    "mass = np.maximum(mass, 0) / np.max(mass)\n",
    "pos = np.array(pos).T\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pos[0], pos[1], pos[2], marker=\".\", alpha=mass)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523919c-72a1-4665-8fc8-81d964aac713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running the gan with different parameters and architectures a evalute the outcomes on small scale simulations.\n",
    "\n",
    "### Not ready yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722100b-6aeb-4d04-8f6b-877b1e56f6bf",
   "metadata": {},
   "source": [
    "Run the simulations with\n",
    "```\n",
    "python main.py\n",
    "```\n",
    "\n",
    "Options are read from `defaults.py` and overridden by options in an `options.py` file, if present. So copy and rename `defaults.py` without modifying it and change options in `options.py`.\n",
    "\n",
    "If an option is a list, e.g. `generator_depth = [4, 5, 6]`, the program will run multiple times with different parameters, logging the results. Attention: making to many parameters into a list may take long time because the program will run every combination in the cartesian product.\n",
    "\n",
    "Process is logged in `last_run.log` and metrics about the runs are saved in `last_run.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6b6a7-63f8-4862-85d0-bb0475ba0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "from random import randint, choice\n",
    "import torch\n",
    "from importlib import reload\n",
    "import os\n",
    "import glob\n",
    "from types import SimpleNamespace\n",
    "import logging\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f2373-4411-424b-8ce5-1063888d018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = pd.read_csv(\"run_f60d59e38c0aa769a21e60b05bb7c89f/global_metrics.csv\")\n",
    "g = sb.FacetGrid(gm, col=\"discriminator\")\n",
    "g.map(sb.lineplot, \"epoch\", \"l1\")\n",
    "# g.map(sb.scatterplot, \"epoch\", \"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842947df-a2ca-4610-86a1-dfe53fb794ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.default_rng(42).random((20, 3))\n",
    "df = pd.DataFrame(data, columns=[\"time\", \"height\", \"width\"])\n",
    "df[[\"time\", \"width\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c3646-ec9b-4e72-b1ed-3ac976654966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baryons",
   "language": "python",
   "name": "baryons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
